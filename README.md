# L1-AND-L2-Regularisation
This is a set of data created from imaginary data of house prices in an urban environment - Paris. I recommend using this dataset for educational purposes, for practice and to acquire the necessary knowledge. What I'm trying to do next is to create a classification dataset with same data from this dataset, I'll add a new column for class attribute ofc. Here is a classification dataset 


Imagine you're in a game where you're trying to guess how much a toy costs in a toy store. You have some clues, like the toy's size, color, and brand. But sometimes, too many clues can make it hard to guess correctly.

Now, in math, when we're trying to guess something (like a toy's price) based on clues (like size, color, and brand), we use something called "regression" to help us make the best guess.

But here's the tricky part: If we have too many clues, our guess might become too complicated. So, we use special rules to keep our guess simple.

Ridge Regression (L2):
Ridge regression is like saying, "Okay, I want to use all the clues, but I don't want to rely too heavily on any one clue." It helps keep our guess simple by making sure no single clue (like size or color) gets too much importance. It's like spreading out your bets evenly so you don't get too focused on one thing.

Lasso Regression (L1):
Lasso regression is even stricter. It's like saying, "I only want to use the most important clues, and I'll ignore the rest if they're not helpful enough." So, if a clue (like the toy's brand) isn't really helping, Lasso might decide to stop using it altogether. This makes our guess even simpler by cutting out less useful clues.

In summary:

Ridge (L2): Uses all clues but keeps them from being too strong.
Lasso (L1): Uses only the most important clues and ignores the rest if they're not useful enough.
So, both methods help us make better guesses by keeping things simple, but they do it in slightly different ways!






